{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc850df9-e407-45d0-830a-fef5e155619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#below is the forcasting method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f26a647-6eb2-4ac2-91a0-3e0f3d4012a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n",
      "Converting timestamps and setting index...\n",
      "\n",
      "Cleaned Column Names:\n",
      "Index(['Unnamed_0', 'Holding_Time_Big', 'Holding_Time_Ock', 'Holding_Time_Bov',\n",
      "       'Holding_Time_Lam', 'Big', 'Ock', 'Bov', 'Lam', 'Decimal_Hours', 'day',\n",
      "       'month', 'year', 'day_of_week', 'Day_0', 'Day_1', 'Day_2', 'Day_3',\n",
      "       'Day_4', 'Day_5', 'Day_6', 'Big_Max', 'Big_Min', 'Ock_Max', 'Ock_Min',\n",
      "       'Bov_Max', 'Bov_Min', 'Lam_Max', 'Lam_Min', 'WTC_L', 'WTC_M', 'WTC_H',\n",
      "       'WTC_J', 'Engine_Jet', 'Engine_Turboprop_shaft', 'Runway_09L',\n",
      "       'Runway_09R', 'Runway_27L', 'Runway_27R', 'No_of_Landings_1HR',\n",
      "       'No_stack', 'ceiling', 'wind', 'precip', 'freezing', 'phenomena',\n",
      "       'wind_dir', 'wind_speed', 'Crosswind_Component', 'Headwind_Component',\n",
      "       'departures_delayIndex', 'arrivals_numCancelled',\n",
      "       'arrivals_delayIndex'],\n",
      "      dtype='object')\n",
      "Frequency is not set. Attempting to set it...\n",
      "Frequency set to: <15 * Minutes>\n",
      "Preparing the data...\n",
      "Dropping rows with NaN values in target columns...\n",
      "Splitting data into training and testing sets...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sktime.forecasting.compose import make_reduction\n",
    "from sktime.forecasting.model_selection import ExpandingWindowSplitter, ForecastingGridSearchCV\n",
    "from sktime.performance_metrics.forecasting import MeanAbsolutePercentageError\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re  # Importing regular expressions\n",
    "from sklearn.metrics import mean_squared_error  # Importing MSE from sklearn\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "print(\"Loading the dataset...\")\n",
    "data = pd.read_csv(\"['EGLL']_2023-04-01_00-00-00_till_2023-08-29_00-00-00_stack.csv\")\n",
    "\n",
    "# Ensure the timestamp is in datetime format and set as index\n",
    "print(\"Converting timestamps and setting index...\")\n",
    "data['TimeStamp'] = pd.to_datetime(data['TimeStamp'])\n",
    "data.set_index('TimeStamp', inplace=True)\n",
    "\n",
    "# Step 2: Clean initial column names to remove spaces or special characters\n",
    "data.columns = [re.sub(r'\\W+', '_', col) for col in data.columns]\n",
    "print(\"\\nCleaned Column Names:\")\n",
    "print(data.columns)\n",
    "\n",
    "# Check if frequency is missing and set it explicitly\n",
    "if data.index.freq is None:\n",
    "    print(\"Frequency is not set. Attempting to set it...\")\n",
    "    try:\n",
    "        data = data.asfreq('15min', method='pad')  # Adjust '15min' to your actual time interval\n",
    "        print(f\"Frequency set to: {data.index.freq}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting frequency: {e}\")\n",
    "\n",
    "# Step 2: Prepare the data\n",
    "print(\"Preparing the data...\")\n",
    "holding_columns = ['Holding_Time_Big', 'Holding_Time_Ock', 'Holding_Time_Bov', 'Holding_Time_Lam']\n",
    "\n",
    "# Handle missing values by dropping rows with NaNs in target columns\n",
    "print(\"Dropping rows with NaN values in target columns...\")\n",
    "data.dropna(subset=holding_columns, inplace=True)\n",
    "\n",
    "# Split data into training and testing sets using train_test_split\n",
    "print(\"Splitting data into training and testing sets...\")\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27db1545-f189-4ea7-bf23-ce694bc5af0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting grid search for each holding column...\n",
      "\n",
      "Processing column: Holding_Time_Big\n",
      "Creating forecaster for Holding_Time_Big...\n",
      "Setting up cross-validation for Holding_Time_Big...\n",
      "Starting grid search for Holding_Time_Big...\n",
      "Fitting the model for Holding_Time_Big...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001204 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 660\n",
      "[LightGBM] [Info] Number of data points in the train set: 11581, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 0.926949\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000542 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 660\n",
      "[LightGBM] [Info] Number of data points in the train set: 11581, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 0.926949\n",
      "Best params for Holding_Time_Big: {'window_length': 15}\n",
      "Forecasting for Holding_Time_Big...\n",
      "Calculating performance metrics for Holding_Time_Big...\n",
      "MAE for Holding_Time_Big: 0.9238863752847736\n",
      "MAPE for Holding_Time_Big: 1.9511092681140363\n",
      "MSE for Holding_Time_Big: 4.2879630143769285\n",
      "\n",
      "Processing column: Holding_Time_Ock\n",
      "Creating forecaster for Holding_Time_Ock...\n",
      "Setting up cross-validation for Holding_Time_Ock...\n",
      "Starting grid search for Holding_Time_Ock...\n",
      "Fitting the model for Holding_Time_Ock...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001233 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1025\n",
      "[LightGBM] [Info] Number of data points in the train set: 11571, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score 0.984746\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001030 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1025\n",
      "[LightGBM] [Info] Number of data points in the train set: 11571, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score 0.984746\n",
      "Best params for Holding_Time_Ock: {'window_length': 25}\n",
      "Forecasting for Holding_Time_Ock...\n",
      "Calculating performance metrics for Holding_Time_Ock...\n",
      "MAE for Holding_Time_Ock: 1.5691793281600377\n",
      "MAPE for Holding_Time_Ock: 1.8389784968802056\n",
      "MSE for Holding_Time_Ock: 3.588015627333372\n",
      "\n",
      "Processing column: Holding_Time_Bov\n",
      "Creating forecaster for Holding_Time_Bov...\n",
      "Setting up cross-validation for Holding_Time_Bov...\n",
      "Starting grid search for Holding_Time_Bov...\n",
      "Fitting the model for Holding_Time_Bov...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000341 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1225\n",
      "[LightGBM] [Info] Number of data points in the train set: 11571, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score 2.434664\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000314 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1225\n",
      "[LightGBM] [Info] Number of data points in the train set: 11571, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score 2.434664\n",
      "Best params for Holding_Time_Bov: {'window_length': 25}\n",
      "Forecasting for Holding_Time_Bov...\n",
      "Calculating performance metrics for Holding_Time_Bov...\n",
      "MAE for Holding_Time_Bov: 3.6158076907027636\n",
      "MAPE for Holding_Time_Bov: 1.7887093633119289\n",
      "MSE for Holding_Time_Bov: 14.29116325287274\n",
      "\n",
      "Processing column: Holding_Time_Lam\n",
      "Creating forecaster for Holding_Time_Lam...\n",
      "Setting up cross-validation for Holding_Time_Lam...\n",
      "Starting grid search for Holding_Time_Lam...\n",
      "Fitting the model for Holding_Time_Lam...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000858 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 900\n",
      "[LightGBM] [Info] Number of data points in the train set: 11576, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 1.096752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000783 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 900\n",
      "[LightGBM] [Info] Number of data points in the train set: 11576, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 1.096752\n",
      "Best params for Holding_Time_Lam: {'window_length': 20}\n",
      "Forecasting for Holding_Time_Lam...\n",
      "Calculating performance metrics for Holding_Time_Lam...\n",
      "MAE for Holding_Time_Lam: 1.770949249344608\n",
      "MAPE for Holding_Time_Lam: 1.809931057340787\n",
      "MSE for Holding_Time_Lam: 4.342238511099436\n",
      "\n",
      "Starting future predictions for each holding column...\n",
      "Forecasting future intervals for Holding_Time_Big...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000811 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 660\n",
      "[LightGBM] [Info] Number of data points in the train set: 11581, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 0.926949\n",
      "Forecasting future intervals for Holding_Time_Ock...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000803 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1025\n",
      "[LightGBM] [Info] Number of data points in the train set: 11571, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score 0.984746\n",
      "Forecasting future intervals for Holding_Time_Bov...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000182 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1225\n",
      "[LightGBM] [Info] Number of data points in the train set: 11571, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score 2.434664\n",
      "Forecasting future intervals for Holding_Time_Lam...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000770 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 900\n",
      "[LightGBM] [Info] Number of data points in the train set: 11576, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 1.096752\n",
      "\n",
      "Future predictions for Holding_Time_Big:\n",
      "2023-07-30 19:15:00    0.161454\n",
      "2023-07-30 19:30:00    0.292598\n",
      "2023-07-30 19:45:00    0.367124\n",
      "2023-07-30 20:00:00    0.367124\n",
      "2023-07-30 20:15:00    0.367124\n",
      "2023-07-30 20:30:00    0.367124\n",
      "2023-07-30 20:45:00    0.367124\n",
      "2023-07-30 21:00:00    0.367124\n",
      "2023-07-30 21:15:00    0.367124\n",
      "2023-07-30 21:30:00    0.367124\n",
      "2023-07-30 21:45:00    0.367124\n",
      "2023-07-30 22:00:00    0.367124\n",
      "2023-07-30 22:15:00    0.367124\n",
      "2023-07-30 22:30:00    0.367124\n",
      "2023-07-30 22:45:00    0.367124\n",
      "2023-07-30 23:00:00    0.367124\n",
      "Freq: 15min, Name: Holding_Time_Big, dtype: float64\n",
      "\n",
      "Future predictions for Holding_Time_Ock:\n",
      "2023-07-30 19:15:00    0.228637\n",
      "2023-07-30 19:30:00    0.569717\n",
      "2023-07-30 19:45:00    0.569717\n",
      "2023-07-30 20:00:00    0.569717\n",
      "2023-07-30 20:15:00    0.647131\n",
      "2023-07-30 20:30:00    0.834963\n",
      "2023-07-30 20:45:00    1.547166\n",
      "2023-07-30 21:00:00    1.908821\n",
      "2023-07-30 21:15:00    2.016566\n",
      "2023-07-30 21:30:00    2.016566\n",
      "2023-07-30 21:45:00    2.003060\n",
      "2023-07-30 22:00:00    1.944452\n",
      "2023-07-30 22:15:00    1.957779\n",
      "2023-07-30 22:30:00    1.810939\n",
      "2023-07-30 22:45:00    1.749725\n",
      "2023-07-30 23:00:00    1.678751\n",
      "Freq: 15min, Name: Holding_Time_Ock, dtype: float64\n",
      "\n",
      "Future predictions for Holding_Time_Bov:\n",
      "2023-07-30 19:15:00    0.227588\n",
      "2023-07-30 19:30:00    0.564842\n",
      "2023-07-30 19:45:00    0.564842\n",
      "2023-07-30 20:00:00    0.792815\n",
      "2023-07-30 20:15:00    1.320464\n",
      "2023-07-30 20:30:00    2.249095\n",
      "2023-07-30 20:45:00    3.014739\n",
      "2023-07-30 21:00:00    3.588105\n",
      "2023-07-30 21:15:00    3.799198\n",
      "2023-07-30 21:30:00    4.158407\n",
      "2023-07-30 21:45:00    4.603161\n",
      "2023-07-30 22:00:00    4.761156\n",
      "2023-07-30 22:15:00    5.080447\n",
      "2023-07-30 22:30:00    4.889585\n",
      "2023-07-30 22:45:00    4.825235\n",
      "2023-07-30 23:00:00    5.150997\n",
      "Freq: 15min, Name: Holding_Time_Bov, dtype: float64\n",
      "\n",
      "Future predictions for Holding_Time_Lam:\n",
      "2023-07-30 19:15:00    0.151362\n",
      "2023-07-30 19:30:00    0.653166\n",
      "2023-07-30 19:45:00    0.678188\n",
      "2023-07-30 20:00:00    0.805354\n",
      "2023-07-30 20:15:00    1.510724\n",
      "2023-07-30 20:30:00    2.325818\n",
      "2023-07-30 20:45:00    2.275240\n",
      "2023-07-30 21:00:00    2.275240\n",
      "2023-07-30 21:15:00    2.474382\n",
      "2023-07-30 21:30:00    2.474382\n",
      "2023-07-30 21:45:00    2.667847\n",
      "2023-07-30 22:00:00    2.594697\n",
      "2023-07-30 22:15:00    2.594697\n",
      "2023-07-30 22:30:00    2.483005\n",
      "2023-07-30 22:45:00    2.226757\n",
      "2023-07-30 23:00:00    1.878231\n",
      "Freq: 15min, Name: Holding_Time_Lam, dtype: float64\n",
      "\n",
      "Process completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Define the forecaster and grid search functions\n",
    "\n",
    "# Function to create a forecaster\n",
    "def create_forecaster(window_length=5):\n",
    "    regressor = lgb.LGBMRegressor()\n",
    "    forecaster = make_reduction(regressor, window_length=window_length, strategy=\"recursive\")\n",
    "    return forecaster\n",
    "\n",
    "# Function to perform grid search and forecast\n",
    "def grid_search_forecaster(train, test, target_column, param_grid):\n",
    "    print(f\"Creating forecaster for {target_column}...\")\n",
    "    forecaster = create_forecaster()\n",
    "\n",
    "    # Cross-validation setup\n",
    "    print(f\"Setting up cross-validation for {target_column}...\")\n",
    "    cv = ExpandingWindowSplitter(initial_window=int(len(train) * 0.7))\n",
    "\n",
    "    # Grid search\n",
    "    print(f\"Starting grid search for {target_column}...\")\n",
    "    gscv = ForecastingGridSearchCV(\n",
    "        forecaster, strategy=\"refit\", cv=cv, param_grid=param_grid,\n",
    "        scoring=MeanAbsolutePercentageError(symmetric=True)\n",
    "    )\n",
    "\n",
    "    # Fit the model\n",
    "    print(f\"Fitting the model for {target_column}...\")\n",
    "    gscv.fit(train[target_column])\n",
    "    print(f\"Best params for {target_column}: {gscv.best_params_}\")\n",
    "\n",
    "    # Forecasting\n",
    "    print(f\"Forecasting for {target_column}...\")\n",
    "    fh = np.arange(len(test)) + 1\n",
    "    y_pred = gscv.predict(fh=fh)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    print(f\"Calculating performance metrics for {target_column}...\")\n",
    "    mae = np.mean(np.abs(test[target_column] - y_pred))\n",
    "    mape = MeanAbsolutePercentageError(symmetric=True)(test[target_column], y_pred)\n",
    "    mse = mean_squared_error(test[target_column], y_pred)  # MSE calculation\n",
    "\n",
    "    print(f\"MAE for {target_column}: {mae}\")\n",
    "    print(f\"MAPE for {target_column}: {mape}\")\n",
    "    print(f\"MSE for {target_column}: {mse}\")  # Print the MSE\n",
    "\n",
    "    return mae, mape, mse, y_pred, gscv.best_params_\n",
    "\n",
    "# Step 4: Perform grid search for each holding column\n",
    "\n",
    "# Define the parameter grid for window length\n",
    "param_grid = {\n",
    "    \"window_length\": [5, 10, 15, 20, 25, 30]  # Grid search over these window lengths\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "print(\"Starting grid search for each holding column...\")\n",
    "for column in holding_columns:\n",
    "    print(f\"\\nProcessing column: {column}\")\n",
    "    mae, mape, mse, predictions, best_params = grid_search_forecaster(train_data, test_data, column, param_grid)\n",
    "    results[column] = {\"MAE\": mae, \"MAPE\": mape, \"MSE\": mse, \"Predictions\": predictions, \"Best_Window_Length\": best_params[\"window_length\"]}\n",
    "\n",
    "# Step 5: Forecast for multiple time steps into the future\n",
    "\n",
    "# Define the future intervals (15-minute steps for 4 hours)\n",
    "future_intervals = np.arange(1, 17)  # 16 intervals (15, 30, ..., 240 minutes)\n",
    "\n",
    "# Dictionary to store future predictions\n",
    "future_predictions = {}\n",
    "\n",
    "print(\"\\nStarting future predictions for each holding column...\")\n",
    "for column in holding_columns:\n",
    "    print(f\"Forecasting future intervals for {column}...\")\n",
    "    # Use the best forecaster found during grid search\n",
    "    best_forecaster = create_forecaster(window_length=results[column][\"Best_Window_Length\"])\n",
    "    best_forecaster.fit(train_data[column])\n",
    "    \n",
    "    # Predict future intervals\n",
    "    future_pred = best_forecaster.predict(fh=future_intervals)\n",
    "    future_predictions[column] = future_pred\n",
    "\n",
    "# Step 6: Display the future predictions\n",
    "for column in holding_columns:\n",
    "    print(f\"\\nFuture predictions for {column}:\")\n",
    "    print(future_predictions[column])\n",
    "\n",
    "print(\"\\nProcess completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e9f429-0985-4812-86c9-ab72e7fef39b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
