{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be7b0e74-d90e-478f-ae1e-79a20fcf8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f747ff6-1aea-4ce6-b107-66d56716b28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metar import Metar\n",
    "\n",
    "\n",
    "def bad_weather_classes(obs: str):\n",
    "    metar = Metar.Metar(obs, strict=False)\n",
    "    ceiling = _get_visibility_ceiling_coef(metar)\n",
    "    wind = _get_wind_coef(metar)\n",
    "    precip = _get_precipitation_coef(metar)\n",
    "    freezing = _get_freezing_coef(metar)\n",
    "    phenomena = _get_dangerous_phenomena_coef(metar)\n",
    "    wspeed,wdir= _get_wind_speed_dir(metar)\n",
    "\n",
    "    return (ceiling, wind, precip, freezing, phenomena,wdir,wspeed)\n",
    "\n",
    "\n",
    "def _get_dangerous_phenomena_coef(metar: Metar.Metar):\n",
    "    phenomena, showers = __dangerous_weather(metar.weather)\n",
    "    cb, tcu, ts = __dangerous_clouds(metar.sky)\n",
    "    if showers is not None and showers > 0:\n",
    "        if cb == 12:\n",
    "            ts = 18 if showers == 1 else 24\n",
    "\n",
    "        if cb == 10 or tcu == 10:\n",
    "            ts = 12 if showers == 1 else 20\n",
    "\n",
    "        if cb == 6 or tcu == 8:\n",
    "            ts = 10 if showers == 1 else 15\n",
    "\n",
    "        if cb == 4 or tcu == 5:\n",
    "            ts = 8 if showers == 1 else 12\n",
    "\n",
    "        if tcu == 3:\n",
    "            ts = 4 if showers == 1 else 6\n",
    "\n",
    "    return max(i for i in [phenomena, cb, tcu, ts] if i is not None)\n",
    "\n",
    "\n",
    "def __dangerous_weather(weather: []):\n",
    "    phenomena = None\n",
    "    showers = None\n",
    "    for intensity, desc, precip, obs, other in weather:\n",
    "        __phenomena = 0\n",
    "        __showers = 0\n",
    "        if other in [\"FC\", \"DS\", \"SS\"] or obs in [\"VA\", \"SA\"] or precip in [\"GR\", \"PL\"]:\n",
    "            __phenomena = 24\n",
    "\n",
    "        if desc == \"TS\":\n",
    "            __phenomena = 30 if intensity == \"+\" else 24\n",
    "\n",
    "        if precip == \"GS\":\n",
    "            __phenomena = 18\n",
    "\n",
    "        if phenomena is None or __phenomena > phenomena:\n",
    "            phenomena = __phenomena\n",
    "\n",
    "        if desc == \"SH\":\n",
    "            __showers = 1 if intensity == \"-\" else 2\n",
    "\n",
    "        if showers is None or __showers > showers:\n",
    "            showers = __showers\n",
    "\n",
    "    return (phenomena, showers)\n",
    "\n",
    "\n",
    "def __dangerous_clouds(sky: []):\n",
    "    cb = 0\n",
    "    tcu = 0\n",
    "    for cover, height, cloud in sky:\n",
    "        __cb = 0\n",
    "        __tcu = 0\n",
    "        if cover == \"OVC\":\n",
    "            if cloud == \"TCU\":\n",
    "                __tcu = 10\n",
    "\n",
    "            if cloud == \"CB\":\n",
    "                __cb = 12\n",
    "\n",
    "        if cover == \"BKN\":\n",
    "            if cloud == \"TCU\":\n",
    "                __tcu = 8\n",
    "\n",
    "            if cloud == \"CB\":\n",
    "                __cb = 10\n",
    "\n",
    "        if cover == \"SCT\":\n",
    "            if cloud == \"TCU\":\n",
    "                __tcu = 5\n",
    "\n",
    "            if cloud == \"CB\":\n",
    "                __cb = 6\n",
    "\n",
    "        if cover == \"FEW\":\n",
    "            if cloud == \"TCU\":\n",
    "                __tcu = 3\n",
    "\n",
    "            if cloud == \"CB\":\n",
    "                __cb = 4\n",
    "\n",
    "        if __cb > cb:\n",
    "            cb = __cb\n",
    "\n",
    "        if __tcu > tcu:\n",
    "            tcu = __tcu\n",
    "\n",
    "    return (cb, tcu, None)\n",
    "\n",
    "\n",
    "def _get_wind_coef(metar: Metar.Metar):\n",
    "    spd = metar.wind_speed.value() if metar.wind_speed is not None else None\n",
    "    gusts = metar.wind_gust.value() if metar.wind_gust is not None else None\n",
    "    coef = 0\n",
    "\n",
    "    if spd is not None:\n",
    "        if 16 <= spd <= 20:\n",
    "            coef = 1\n",
    "\n",
    "        if 21 <= spd <= 30:\n",
    "            coef = 2\n",
    "\n",
    "        if spd > 30:\n",
    "            coef = 4\n",
    "\n",
    "        if gusts is not None:\n",
    "            coef += 1\n",
    "\n",
    "    return coef\n",
    "\n",
    "\n",
    "def _get_wind_speed_dir(metar: Metar.Metar):\n",
    "    wspd = metar.wind_speed.value()\n",
    "    wdir = metar.wind_dir\n",
    "\n",
    "    if not (wdir==None):\n",
    "        wdir=wdir.value()\n",
    "    else:\n",
    "        wdir=0\n",
    "    return wspd,wdir\n",
    "\n",
    "\n",
    "def _get_precipitation_coef(metar: Metar.Metar):\n",
    "    coef = 0\n",
    "    for intensity, desc, precip, obs, other in metar.weather:\n",
    "        __coef = 0\n",
    "        if desc == \"FZ\":\n",
    "            __coef = 3\n",
    "\n",
    "        if precip == \"SN\":\n",
    "            __coef = 2 if intensity == \"-\" else 3\n",
    "\n",
    "        if precip == \"SG\" or (precip == \"RA\" and intensity == \"+\"):\n",
    "            __coef = 2\n",
    "\n",
    "        if precip in [\"RA\", \"UP\", \"IC\", \"DZ\"]:\n",
    "            __coef = 1\n",
    "\n",
    "        if __coef > coef:\n",
    "            coef = __coef\n",
    "\n",
    "    return coef\n",
    "\n",
    "\n",
    "def _get_freezing_coef(metar: Metar.Metar):\n",
    "    tt = metar.temp.value() if metar.temp is not None else None\n",
    "    dp = metar.dewpt.value() if metar.dewpt is not None else None\n",
    "    moisture = None\n",
    "    for intensity, desc, precip, obs, other in metar.weather:\n",
    "        __moisture = 0\n",
    "        if desc == \"FZ\":\n",
    "            __moisture = 5\n",
    "\n",
    "        if precip == \"SN\":\n",
    "            __moisture = 4 if intensity == \"-\" else 5\n",
    "\n",
    "        if precip in [\"SG\", \"RASN\"] or (precip == \"RA\" and intensity == \"+\") or obs == \"BR\":\n",
    "            __moisture = 4\n",
    "\n",
    "        if precip in [\"DZ\", \"IC\", \"RA\", \"UP\", \"GR\", \"GS\", \"PL\"] or obs == \"FG\":\n",
    "            __moisture = 3\n",
    "\n",
    "        if moisture is None or __moisture > moisture:\n",
    "            moisture = __moisture\n",
    "\n",
    "    if tt is not None and tt <= 3 and moisture == 5:\n",
    "        return 4\n",
    "\n",
    "    if tt is not None and tt <= -15 and moisture is not None:\n",
    "        return 4\n",
    "\n",
    "    if tt is not None and tt <= 3 and moisture == 4:\n",
    "        return 3\n",
    "\n",
    "    if tt is not None and tt <= 3 and (moisture == 3 or (tt - dp) < 3):\n",
    "        return 1\n",
    "\n",
    "    if tt is not None and tt <= 3 and moisture is None:\n",
    "        return 0\n",
    "\n",
    "    if tt is not None and tt > 3 and moisture is not None:\n",
    "        return 0\n",
    "\n",
    "    if tt is not None and tt > 3 and (moisture is None or (tt - dp) >= 3):\n",
    "        return 0\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _get_visibility_ceiling_coef(metar: Metar.Metar):\n",
    "    vis = __get_visibility(metar)\n",
    "    is_covered, cld_base = __get_ceiling(metar)\n",
    "\n",
    "    if vis is not None and cld_base is not None:\n",
    "        if (vis <= 325) or (is_covered and cld_base <= 50):\n",
    "            return 5\n",
    "\n",
    "        if (350 <= vis <= 500) or (is_covered and 100 <= cld_base <= 150):\n",
    "            return 4\n",
    "\n",
    "        if (550 <= vis <= 750) or (is_covered and 200 <= cld_base <= 250):\n",
    "            return 2\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "def __get_ceiling(metar: Metar.Metar):\n",
    "    cld_cover = False\n",
    "    cld_base = None\n",
    "    for cover, height, cloud in metar.sky:\n",
    "        if cover in [\"BKN\", \"OVC\"]:\n",
    "            cld_cover = True\n",
    "\n",
    "        if height is not None and (cld_base is None or cld_base > height.value()):\n",
    "            cld_base = height.value()\n",
    "\n",
    "    return (cld_cover, cld_base)\n",
    "\n",
    "\n",
    "def __get_visibility(metar: Metar.Metar):\n",
    "    vis = None\n",
    "    if metar.vis is not None:\n",
    "        vis = metar.vis.value()\n",
    "    rvr = None\n",
    "    for name, low, high, unit in metar.runway:\n",
    "        if rvr is None or rvr > low.value():\n",
    "            rvr = low.value()\n",
    "\n",
    "    if rvr is not None and rvr < 1500:\n",
    "        vis = rvr\n",
    "\n",
    "    return vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1665d8a5-4c7a-40d3-b7a8-e77f29c29cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example script that scrapes data from the IEM ASOS download service.\n",
    "Requires: Python 3\n",
    "\"\"\"\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Number of attempts to download data\n",
    "MAX_ATTEMPTS = 6\n",
    "# HTTPS here can be problematic for installs that don't have Lets Encrypt CA\n",
    "SERVICE = \"http://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?\"\n",
    "\n",
    "df1= None\n",
    "startts= None\n",
    "endts= None\n",
    "stations=None\n",
    "\n",
    "def download_data(uri):\n",
    "    \"\"\"Fetch the data from the IEM\n",
    "    The IEM download service has some protections in place to keep the number\n",
    "    of inbound requests in check.  This function implements an exponential\n",
    "    backoff to keep individual downloads from erroring.\n",
    "    Args:\n",
    "      uri (string): URL to fetch\n",
    "    Returns:\n",
    "      string data\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    while attempt < MAX_ATTEMPTS:\n",
    "        try:\n",
    "            data = urlopen(uri, timeout=300).read().decode(\"utf-8\")\n",
    "            if data is not None and not data.startswith(\"ERROR\"):\n",
    "                return data\n",
    "        except Exception as exp:\n",
    "            print(f\"download_data({uri}) failed with {exp}\")\n",
    "            time.sleep(5)\n",
    "        attempt += 1\n",
    "\n",
    "    print(\"Exhausted attempts to download, returning empty data\")\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def main_weather(startts,endts,stations):\n",
    "    \"\"\"Our main method\"\"\"\n",
    "    #global df1 , endts, startts , stations\n",
    "    # timestamps in UTC to request data for\n",
    "    #startts = datetime.datetime(2021, 12, 1)\n",
    "    #endts = datetime.datetime(2022, 2, 1)\n",
    "\n",
    "    service = SERVICE + \"data=all&tz=Etc/UTC&format=comma&latlon=yes&\"\n",
    "\n",
    "    service += startts.strftime(\"year1=%Y&month1=%m&day1=%d&\")\n",
    "    service += endts.strftime(\"year2=%Y&month2=%m&day2=%d&\")\n",
    "\n",
    "    # Specify the airport station code\n",
    "    #stations = [\"EGLL\"]\n",
    "    for station in stations:\n",
    "        uri = f\"{service}&station={station}\"\n",
    "        print(f\"Downloading: {station}\")\n",
    "        data = download_data(uri)\n",
    "        outfn = f\"{station}_{startts:%Y%m%d%H%M}_{endts:%Y%m%d%H%M}.txt\"\n",
    "        with open(outfn, \"w\", encoding=\"ascii\") as fh:\n",
    "            fh.write(data)\n",
    "        df1 = pd.read_csv(f\"C:\\\\Users\\mic__\\{station}_{startts:%Y%m%d%H%M}_{endts:%Y%m%d%H%M}.txt\", sep = ',', header=5)\n",
    "\n",
    "\n",
    "    df2=df1[[\"metar\",\"valid\"]]\n",
    "    df2\n",
    "\n",
    "    final = pd.DataFrame(columns=['metar', 'valid', 'ceiling', 'wind', 'precip', 'freezing', 'phenomena','wind dir','wind speed'])\n",
    "\n",
    "    for index, row in df2.iterrows():\n",
    "        metar = row[\"metar\"]\n",
    "        valid = row[\"valid\"]\n",
    "        #print(metar)\n",
    "        (ceiling, wind, precip, freezing, phenomena,wdir,wspeed) = bad_weather_classes(row[\"metar\"])\n",
    "        #comment for wind direction and speed\n",
    "\n",
    "        if ceiling is not None and wind is not None and precip is not None and freezing is not None and phenomena is not None:\n",
    "            row_df = pd.DataFrame({'metar': metar, 'valid': valid, 'ceiling': ceiling, 'wind': wind, 'precip': precip, 'freezing': freezing, 'phenomena': phenomena,'wind dir':wdir,'wind speed':wspeed}, index=[0])\n",
    "            final = pd.concat([final, row_df], ignore_index=True)\n",
    "\n",
    "    final.to_csv(f\"{stations}_{startts:%Y%m%d%H%M}_{endts:%Y%m%d%H%M}_final.csv\", index=False)\n",
    "\n",
    "    final['valid'] = pd.to_datetime(final['valid'])\n",
    "\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc716e45-7045-4d30-88b9-a9841d3c72b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stack_df is the needed df which has timestamps 15 mins interval\n",
    "#day_df is the data we have\n",
    "#stack id  is id\n",
    "def stack_count(stack_df,day_df,stack_id):\n",
    "    # day_stack_dfis is the data we have for specific stack of a day\n",
    "    day_stack_df = day_df[day_df[stack_id] == 1]\n",
    "    \n",
    "\n",
    "    for index, row in stack_df.iterrows():\n",
    "        specific_timestamp=row['TimeStamp']\n",
    "        \n",
    "        filtered_day_15min = day_stack_df[day_stack_df['Time_rounded'] == specific_timestamp]\n",
    "        \n",
    "        id_sum_value = filtered_day_15min[stack_id].sum()\n",
    "        stack_df.at[index, stack_id]=id_sum_value\n",
    "        \n",
    "        id_holding_mean = filtered_day_15min['Holding Time (minutes)'].mean(numeric_only=True)\n",
    "        stack_df.at[index, f'Holding Time {stack_id}']=id_holding_mean \n",
    "\n",
    "        id_max_value = filtered_day_15min['Holding Time (minutes)'].max()\n",
    "        stack_df.at[index, f'{stack_id} Max']=id_max_value\n",
    "\n",
    "        id_min_value = filtered_day_15min['Holding Time (minutes)'].min()\n",
    "        stack_df.at[index, f'{stack_id} Min']=id_min_value\n",
    "        \n",
    "        \n",
    "        \n",
    "    return stack_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "307153af-5f58-493f-a9b6-2eee44623c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stack_df is the needed df which has timestamps 15 mins interval\n",
    "#day_df is the data we have\n",
    "#stack id  is id\n",
    "def stack_no_fix_def(day_df,stack_id):\n",
    "    # day_stack_dfis is the data we have for specific stack of a day\n",
    "    stack_no_fix = day_df[day_df[stack_id] == 0]\n",
    "    return stack_no_fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55da12e4-6aeb-4ebe-80b7-9ad2ecfb2622",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stack_df is the needed df which has timestamps 15 mins interval\n",
    "#day_df is the data we have\n",
    "#stack id  is id\n",
    "def stacks_count(stack_df,day_df):\n",
    "    for index, row in stack_df.iterrows():\n",
    "        specific_timestamp=row['TimeStamp']\n",
    "        \n",
    "        filtered_day_15min = day_df[day_df['Time_rounded'] == specific_timestamp]\n",
    "        \n",
    "        WTC_L = filtered_day_15min['WTC L'].sum()\n",
    "        stack_df.at[index, 'WTC L']=WTC_L\n",
    "        \n",
    "        WTC_M = filtered_day_15min['WTC M'].sum()\n",
    "        stack_df.at[index, 'WTC M']=WTC_M\n",
    "        \n",
    "        WTC_H = filtered_day_15min['WTC H'].sum()\n",
    "        stack_df.at[index, 'WTC H']=WTC_H\n",
    "        \n",
    "        WTC_J = filtered_day_15min['WTC J'].sum()\n",
    "        stack_df.at[index, 'WTC J']=WTC_J\n",
    "        \n",
    "        Engine_Jet = filtered_day_15min['Engine Jet'].sum()\n",
    "        stack_df.at[index, 'Engine Jet']=Engine_Jet\n",
    "        \n",
    "        Engine_Turboprop = filtered_day_15min['Engine Turboprop/shaft'].sum()\n",
    "        stack_df.at[index, 'Engine Turboprop/shaft']=Engine_Turboprop\n",
    "\n",
    "        Runway09L = filtered_day_15min['Runway 09L'].sum()\n",
    "        stack_df.at[index, 'Runway 09L']=Runway09L\n",
    "\n",
    "        Runway09R = filtered_day_15min['Runway 09R'].sum()\n",
    "        stack_df.at[index, 'Runway 09R']=Runway09R\n",
    "\n",
    "        Runway27L = filtered_day_15min['Runway 27L'].sum()\n",
    "        stack_df.at[index, 'Runway 27L']=Runway27L\n",
    "        \n",
    "        Runway27R = filtered_day_15min['Runway 27R'].sum()\n",
    "        stack_df.at[index, 'Runway 27R']=Runway27R\n",
    "       \n",
    "       \n",
    "    \n",
    "    return stack_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04e4dc59-0aae-420b-816f-fefe4fc69161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_acute_angle(angle1, angle2):\n",
    "\n",
    "    # Calculate the absolute difference between the angles\n",
    "    angle_difference = abs(angle1 - angle2)\n",
    "\n",
    "    # Take the minimum of the absolute difference and its complement\n",
    "    acute_angle = min(angle_difference, 360 - angle_difference)\n",
    "\n",
    "    return acute_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b1c1588-cda2-431e-874a-c4363d45a9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_crosswind(stack_df):\n",
    "    \n",
    "    runway_columns_value = stack_df.filter(regex='^Runway').copy()\n",
    "    runway_columns = stack_df.filter(regex='^Runway').copy()\n",
    "    \n",
    "    # Find the maximum value in each row\n",
    "    runway_columns_value['Max_Value'] = runway_columns_value.max(axis=1)\n",
    "\n",
    "    runway_columns['Max_Column'] = runway_columns.idxmax(axis=1)\n",
    "\n",
    "    \n",
    "    for index, row in runway_columns_value.iterrows():\n",
    "        if row['Max_Value']==0:\n",
    "            stack_df.at[index, 'Crosswind Component']=0\n",
    "            stack_df.at[index, 'Headwind Component']=0\n",
    "        else:\n",
    "            wind_dir=stack_df.at[index,'wind dir']\n",
    "            wind_speed=stack_df.at[index,'wind speed']\n",
    "\n",
    "            if ('09' in runway_columns.at[index,'Max_Column']):\n",
    "                angular_dif= calculate_acute_angle(wind_dir,90)\n",
    "                \n",
    "            else:\n",
    "                angular_dif= calculate_acute_angle(wind_dir,270)\n",
    "\n",
    "            cross_wind = np.round(np.abs(wind_speed * np.sin(np.radians(angular_dif))), 0)\n",
    "            head_wind = np.round(np.abs(wind_speed * np.cos(np.radians(angular_dif))), 0)\n",
    "            \n",
    "            stack_df.at[index,'Crosswind Component']=cross_wind\n",
    "            stack_df.at[index,'Headwind Component']=head_wind\n",
    "    return stack_df\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3e71772-a142-48d2-b205-f6d18fe53535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def No_of_landings (stack_df,day_df):\n",
    "    no_of_landings=0\n",
    "    for index, row in stack_df.iterrows():\n",
    "        specific_timestamp=row['TimeStamp']\n",
    "\n",
    "        filtered_day_15min = day_df[day_df['Time_rounded_last_ts'] == specific_timestamp]\n",
    "\n",
    "        no_of_landings_1 = filtered_day_15min['Time_rounded_last_ts'].count()\n",
    "        no_of_landings_2 = (filtered_day_15min['Time_rounded_last_ts']-pd.to_timedelta(15, unit='m')).count()\n",
    "        no_of_landings_3 = (filtered_day_15min['Time_rounded_last_ts']-pd.to_timedelta(30, unit='m')).count()\n",
    "        no_of_landings_4 = (filtered_day_15min['Time_rounded_last_ts']-pd.to_timedelta(45, unit='m')).count()\n",
    "        no_of_landings=no_of_landings_1+no_of_landings_2+no_of_landings_3+no_of_landings_4\n",
    "        stack_df.at[index, 'No of Landings 1HR']=no_of_landings\n",
    "    return stack_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f80bdcd6-1f38-4524-a69d-460564679008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_no_fix_count(stack_no_fix_day, stack_df):\n",
    "    for index,row in stack_df.iterrows():\n",
    "        specific_timestamp=row['TimeStamp']\n",
    "        filtered_stack_15min = stack_no_fix_day[stack_no_fix_day['Time_rounded_last_ts'] == specific_timestamp]\n",
    "        #print(filtered_stack_15min)\n",
    "        if filtered_stack_15min.empty:\n",
    "            stack_df.at[index, 'No stack']=0\n",
    "        else:\n",
    "            no_stack_total=0\n",
    "            no_stack_total = filtered_stack_15min['Time_rounded_last_ts'].count()\n",
    "            \n",
    "            #time.sleep(60) \n",
    "            stack_df.at[index, 'No stack']=no_stack_total\n",
    "\n",
    "    return stack_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "366a2873-ecd2-4a9d-8cd9-adc266ce38de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original_df=pd.read_csv('EGLL_2023-04-01_00-00_till_2023-08-31_23-30.csv')\n",
    "delay_index_df=pd.read_csv('Delayindex_of_EGLL_from_2023-03-25_01_00_till_2024-03-23_23_30')\n",
    "station=['EGLL']\n",
    "\n",
    "\n",
    "original_df = original_df.sort_values(by='TimeStamp')\n",
    "original_df['Time leaving fix'] = pd.to_datetime(original_df['Time leaving fix'])\n",
    "original_df['Last Timestamp'] = pd.to_datetime(original_df['Last Timestamp'])\n",
    "\n",
    "\n",
    "delay_index_df=delay_index_df.fillna(0)\n",
    "delay_index_df=delay_index_df.drop(columns=[\"Unnamed: 0\",'airportIcao','from_utc','departures_numTotal','departures_numCancelled','departures_medianDelay','arrivals_numTotal','arrivals_medianDelay'])\n",
    "delay_index_df['to_utc'] = pd.to_datetime(delay_index_df['to_utc'] )\n",
    "delay_index_df['to_utc']=delay_index_df['to_utc'].dt.tz_localize(None)\n",
    "delay_index_df = delay_index_df.sort_values(by='to_utc')\n",
    "\n",
    "# Round the timestamps to the nearest 15-minute interval\n",
    "original_df['Time_rounded'] = original_df['Time leaving fix'].dt.ceil(freq='15min')\n",
    "original_df['Time_rounded_last_ts'] = original_df['Last Timestamp'].dt.ceil(freq='15min')\n",
    "#print(original_df)\n",
    "\n",
    "first_date=(original_df['TimeStamp']).iloc[0]\n",
    "\n",
    "# Convert the 'date' column to datetime format\n",
    "first_date = pd.to_datetime(first_date)\n",
    "first_date = first_date.date()\n",
    "\n",
    "last_date=(original_df['TimeStamp']).iloc[-1]\n",
    "# Convert the 'date' column to datetime format\n",
    "last_date = pd.to_datetime(last_date)\n",
    "last_date = last_date.date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4e20903-f3de-4299-9efe-a1acd0b6073d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: EGLL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mic__\\AppData\\Local\\Temp\\ipykernel_37348\\4032102984.py:86: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final = pd.concat([final, row_df], ignore_index=True)\n",
      "C:\\Users\\mic__\\.conda\\envs\\traffic\\lib\\site-packages\\metar\\Metar.py:502: RuntimeWarning: Unparsed groups in body 'PROB30' while processing 'COR EGLL 272020Z AUTO 11010KT 8000 -RA BKN008/// OVC024/// //////TCU 10/09 Q1009 RERA PROB30 TEMPO 4000 RADZ BKN004'\n",
      "  warnings.warn(message, RuntimeWarning)\n",
      "C:\\Users\\mic__\\.conda\\envs\\traffic\\lib\\site-packages\\metar\\Metar.py:502: RuntimeWarning: Unparsed groups in body '13/NOSIG06' while processing 'COR EGLL 262120Z AUTO 08008KT 9999 NCD 13/NOSIG06 Q1031'\n",
      "  warnings.warn(message, RuntimeWarning)\n",
      "C:\\Users\\mic__\\.conda\\envs\\traffic\\lib\\site-packages\\metar\\Metar.py:502: RuntimeWarning: Unparsed groups in body '4000' while processing 'COR EGLL 221950Z AUTO 21013KT 9999 -RA BKN010 OVC015 16/15 Q1006 TEMPO RA 4000'\n",
      "  warnings.warn(message, RuntimeWarning)\n",
      "C:\\Users\\mic__\\.conda\\envs\\traffic\\lib\\site-packages\\metar\\Metar.py:502: RuntimeWarning: Unparsed groups in body 'DZ BKN008' while processing 'COR EGLL 310020Z AUTO 22013KT 9999 OVC010 17/15 Q1009 4000 DZ BKN008'\n",
      "  warnings.warn(message, RuntimeWarning)\n",
      "C:\\Users\\mic__\\.conda\\envs\\traffic\\lib\\site-packages\\metar\\Metar.py:502: RuntimeWarning: Unparsed groups in body '290V35NOSIG0' while processing 'COR EGLL 040220Z AUTO 31004KT 290V35NOSIG0 9999 NCD 15/11 Q1014 NOSIG'\n",
      "  warnings.warn(message, RuntimeWarning)\n",
      "C:\\Users\\mic__\\.conda\\envs\\traffic\\lib\\site-packages\\metar\\Metar.py:502: RuntimeWarning: Unparsed groups in body '4000 SHRA' while processing 'COR EGLL 051250Z 16003KT 100V230 2000 +SHRA VCTS BKN011CB OVC029 17/15 Q1001 RETSRA RESHRA 4000 SHRA'\n",
      "  warnings.warn(message, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for 2023-04-01 Processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mic__\\AppData\\Local\\Temp\\ipykernel_37348\\2365741133.py:62: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  stack_df = pd.concat([stack_df, stack_day_df])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for 2023-04-02 Processed.\n",
      "Data for 2023-04-03 Processed.\n",
      "Data for 2023-04-04 Processed.\n",
      "Data for 2023-04-05 Processed.\n",
      "Data for 2023-04-06 Processed.\n",
      "Data for 2023-04-07 Processed.\n",
      "Data for 2023-04-08 Processed.\n",
      "Data for 2023-04-09 Processed.\n",
      "Data for 2023-04-10 Processed.\n",
      "Data for 2023-04-11 Processed.\n",
      "Data for 2023-04-12 Processed.\n",
      "Data for 2023-04-13 Processed.\n",
      "Data for 2023-04-14 Processed.\n",
      "Data for 2023-04-15 Processed.\n",
      "Data for 2023-04-16 Processed.\n",
      "Data for 2023-04-17 Processed.\n",
      "Data for 2023-04-18 Processed.\n",
      "Data for 2023-04-19 Processed.\n",
      "Data for 2023-04-20 Processed.\n",
      "Data for 2023-04-21 Processed.\n",
      "Data for 2023-04-22 Processed.\n",
      "Data for 2023-04-23 Processed.\n",
      "Data for 2023-04-24 Processed.\n",
      "Data for 2023-04-25 Processed.\n",
      "Data for 2023-04-26 Processed.\n",
      "Data for 2023-04-27 Processed.\n",
      "Data for 2023-04-28 Processed.\n",
      "Data for 2023-04-29 Processed.\n",
      "Data for 2023-04-30 Processed.\n",
      "Data for 2023-05-01 Processed.\n",
      "Data for 2023-05-02 Processed.\n",
      "Data for 2023-05-03 Processed.\n",
      "Data for 2023-05-04 Processed.\n",
      "Data for 2023-05-05 Processed.\n",
      "Data for 2023-05-06 Processed.\n",
      "Data for 2023-05-07 Processed.\n",
      "Data for 2023-05-08 Processed.\n",
      "Data for 2023-05-09 Processed.\n",
      "Data for 2023-05-10 Processed.\n",
      "Data for 2023-05-11 Processed.\n",
      "Data for 2023-05-12 Processed.\n",
      "Data for 2023-05-13 Processed.\n",
      "Data for 2023-05-14 Processed.\n",
      "Data for 2023-05-15 Processed.\n",
      "Data for 2023-05-16 Processed.\n",
      "Data for 2023-05-17 Processed.\n",
      "Data for 2023-05-18 Processed.\n",
      "Data for 2023-05-19 Processed.\n",
      "Data for 2023-05-20 Processed.\n",
      "Data for 2023-05-21 Processed.\n",
      "Data for 2023-05-22 Processed.\n",
      "Data for 2023-05-23 Processed.\n",
      "Data for 2023-05-24 Processed.\n",
      "Data for 2023-05-25 Processed.\n",
      "Data for 2023-05-26 Processed.\n",
      "Data for 2023-05-27 Processed.\n",
      "Data for 2023-05-28 Processed.\n",
      "Data for 2023-05-29 Processed.\n",
      "Data for 2023-05-30 Processed.\n",
      "Data for 2023-05-31 Processed.\n",
      "Data for 2023-06-01 Processed.\n",
      "Data for 2023-06-02 Processed.\n",
      "Data for 2023-06-03 Processed.\n",
      "Data for 2023-06-04 Processed.\n",
      "Data for 2023-06-05 Processed.\n",
      "Data for 2023-06-06 Processed.\n",
      "Data for 2023-06-07 Processed.\n",
      "Data for 2023-06-08 Processed.\n",
      "Data for 2023-06-09 Processed.\n",
      "Data for 2023-06-10 Processed.\n",
      "Data for 2023-06-11 Processed.\n",
      "Data for 2023-06-12 Processed.\n",
      "Data for 2023-06-13 Processed.\n",
      "Data for 2023-06-14 Processed.\n",
      "Data for 2023-06-15 Processed.\n",
      "Data for 2023-06-16 Processed.\n",
      "Data for 2023-06-17 Processed.\n",
      "Data for 2023-06-18 Processed.\n",
      "Data for 2023-06-19 Processed.\n",
      "Data for 2023-06-20 Processed.\n",
      "Data for 2023-06-21 Processed.\n",
      "Data for 2023-06-22 Processed.\n",
      "Data for 2023-06-23 Processed.\n",
      "Data for 2023-06-24 Processed.\n",
      "Data for 2023-06-25 Processed.\n",
      "No data for 2023-06-26. Skipping to next date.\n",
      "No data for 2023-06-27. Skipping to next date.\n",
      "No data for 2023-06-28. Skipping to next date.\n",
      "No data for 2023-06-29. Skipping to next date.\n",
      "No data for 2023-06-30. Skipping to next date.\n",
      "Data for 2023-07-01 Processed.\n",
      "Data for 2023-07-02 Processed.\n",
      "Data for 2023-07-03 Processed.\n",
      "Data for 2023-07-04 Processed.\n",
      "Data for 2023-07-05 Processed.\n",
      "Data for 2023-07-06 Processed.\n",
      "Data for 2023-07-07 Processed.\n",
      "Data for 2023-07-08 Processed.\n",
      "Data for 2023-07-09 Processed.\n",
      "Data for 2023-07-10 Processed.\n",
      "Data for 2023-07-11 Processed.\n",
      "Data for 2023-07-12 Processed.\n",
      "Data for 2023-07-13 Processed.\n",
      "Data for 2023-07-14 Processed.\n",
      "Data for 2023-07-15 Processed.\n",
      "Data for 2023-07-16 Processed.\n",
      "Data for 2023-07-17 Processed.\n",
      "No data for 2023-07-18. Skipping to next date.\n",
      "Data for 2023-07-19 Processed.\n",
      "Data for 2023-07-20 Processed.\n",
      "No data for 2023-07-21. Skipping to next date.\n",
      "Data for 2023-07-22 Processed.\n",
      "Data for 2023-07-23 Processed.\n",
      "Data for 2023-07-24 Processed.\n",
      "Data for 2023-07-25 Processed.\n",
      "Data for 2023-07-26 Processed.\n",
      "Data for 2023-07-27 Processed.\n",
      "No data for 2023-07-28. Skipping to next date.\n",
      "No data for 2023-07-29. Skipping to next date.\n",
      "No data for 2023-07-30. Skipping to next date.\n",
      "No data for 2023-07-31. Skipping to next date.\n",
      "No data for 2023-08-01. Skipping to next date.\n",
      "Data for 2023-08-02 Processed.\n",
      "Data for 2023-08-03 Processed.\n",
      "Data for 2023-08-04 Processed.\n",
      "No data for 2023-08-05. Skipping to next date.\n",
      "Data for 2023-08-06 Processed.\n",
      "Data for 2023-08-07 Processed.\n",
      "Data for 2023-08-08 Processed.\n",
      "No data for 2023-08-09. Skipping to next date.\n",
      "Data for 2023-08-10 Processed.\n",
      "Data for 2023-08-11 Processed.\n",
      "No data for 2023-08-12. Skipping to next date.\n",
      "No data for 2023-08-13. Skipping to next date.\n",
      "No data for 2023-08-14. Skipping to next date.\n",
      "No data for 2023-08-15. Skipping to next date.\n",
      "No data for 2023-08-16. Skipping to next date.\n",
      "No data for 2023-08-17. Skipping to next date.\n",
      "No data for 2023-08-18. Skipping to next date.\n",
      "No data for 2023-08-19. Skipping to next date.\n",
      "No data for 2023-08-20. Skipping to next date.\n",
      "Data for 2023-08-21 Processed.\n",
      "Data for 2023-08-22 Processed.\n",
      "Data for 2023-08-23 Processed.\n",
      "Data for 2023-08-24 Processed.\n",
      "Data for 2023-08-25 Processed.\n",
      "Data for 2023-08-26 Processed.\n",
      "Data for 2023-08-27 Processed.\n",
      "Data for 2023-08-28 Processed.\n",
      "Data for 2023-08-29 Processed.\n"
     ]
    }
   ],
   "source": [
    "range_ap=pd.date_range(first_date,last_date)\n",
    "\n",
    "weather=main_weather(first_date-pd.Timedelta(days=1),last_date,station)\n",
    "weather_sorted = weather.sort_values(by='valid')\n",
    "\n",
    "\n",
    "\n",
    "stack_df = pd.DataFrame(columns=['Holding Time Big','Holding Time Ock','Holding Time Bov','Holding Time Lam','TimeStamp','Big', 'Ock', 'Bov','Lam'])\n",
    "stack_ids=['Big', 'Ock', 'Bov','Lam']\n",
    "\n",
    "\n",
    "\n",
    "for i in range_ap:\n",
    "    date_to_filter=i.date()\n",
    "    day=date_to_filter.day\n",
    "    month=date_to_filter.month\n",
    "\n",
    "    day_df = original_df.loc[(original_df['day'] == day) & (original_df['month'] == month)]\n",
    "\n",
    "    stack_no_fix_day = day_df.copy()\n",
    "    \n",
    "    \n",
    "    if not day_df.empty :\n",
    "\n",
    "        stack_day_df = pd.DataFrame({'TimeStamp': pd.date_range(start=(i+ pd.to_timedelta(15, unit='m')), periods=96, freq='15min')})\n",
    "\n",
    "        \n",
    "        current_time=stack_day_df['TimeStamp']\n",
    "        stack_day_df[f'Decimal Hours'] =(current_time.dt.hour + current_time.dt.minute / 60).round(2)\n",
    "        \n",
    "        stack_day_df['day'] = stack_day_df['TimeStamp'].dt.day\n",
    "        stack_day_df['month'] = stack_day_df['TimeStamp'].dt.month\n",
    "        stack_day_df['year'] = stack_day_df['TimeStamp'].dt.year\n",
    "        stack_day_df['day_of_week'] = stack_day_df['TimeStamp'].dt.weekday # Monday=0, Sunday=6\n",
    "\n",
    "        for index, row in stack_day_df.iterrows():\n",
    "            for dow in [0,1,2,3,4,5,6]:\n",
    "                if stack_day_df.at[index,'day_of_week'] == dow:\n",
    "                    stack_day_df.at[index,f'Day_{dow}']=1\n",
    "                else:\n",
    "                    stack_day_df.at[index,f'Day_{dow}']=0\n",
    "        \n",
    "\n",
    "        \n",
    "        for stack_id in stack_ids:\n",
    "            stack_day_df=stack_count(stack_day_df,day_df,stack_id)\n",
    "            stack_day_df.fillna(0, inplace=True)\n",
    "            stack_day_df[f'Holding Time {stack_id}'] = (stack_day_df[f'Holding Time {stack_id}'] * 2).round() / 2\n",
    "            \n",
    "        stack_day_df=stacks_count(stack_day_df,day_df)\n",
    "        stack_day_df=No_of_landings (stack_day_df,day_df)\n",
    "        \n",
    "        for stack_id in stack_ids:\n",
    "            stack_no_fix_day=stack_no_fix_def(stack_no_fix_day,stack_id)\n",
    "\n",
    "        stack_day_df= stack_no_fix_count(stack_no_fix_day, stack_day_df)\n",
    "        \n",
    "        print(f\"Data for {date_to_filter} Processed.\")\n",
    "\n",
    "        # Concatenate the DataFrames\n",
    "        #stack_df increases by every loop\n",
    "        stack_df = pd.concat([stack_df, stack_day_df])\n",
    "        #stack_df.to_csv('stack_temp_1')\n",
    "    else:\n",
    "        print(f\"No data for {date_to_filter}. Skipping to next date.\")\n",
    "        continue\n",
    "        \n",
    "merged_weather = pd.merge_asof(stack_df, weather_sorted, left_on='TimeStamp', right_on='valid', direction='backward')\n",
    "stack_df=merged_weather\n",
    "stack_df=stack_df.drop(columns=['metar','valid'])\n",
    "stack_df=cal_crosswind(stack_df)\n",
    "\n",
    "merged_delay_index = pd.merge_asof(stack_df, delay_index_df, left_on='TimeStamp', right_on='to_utc', direction='backward')\n",
    "stack_df=merged_delay_index\n",
    "stack_df=stack_df.drop(columns=['to_utc'])\n",
    "\n",
    "#print(stack_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7da1126d-72a5-4f25-97c8-99906c68fbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_date = first_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "last_date = last_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "first_date=first_date.replace(\" \", \"_\").replace(\":\", \"-\").replace('-00+0000','')\n",
    "last_date=last_date.replace(\" \", \"_\").replace(\":\", \"-\").replace('-00+0000','')\n",
    "stack_df.to_csv(f\"{station}_{first_date}_till_{last_date}_stack.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372c257b-07ec-4ddc-9aef-f5a4186aa347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
